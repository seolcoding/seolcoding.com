# 석사학위논문 발표 기획서

> **논문 제목**: 한국어 공공 도메인에서의 On-premise RAG 파이프라인 최적화 연구
> **발표자**: 설동헌
> **소속**: 부경대학교 대학원 ICT교통융합전공
> **지도교수**: 윤상석
> **발표일**: 2025년 12월 11일

---

## 1. 발표의 핵심 메시지 (Take-away)

### 한 줄 요약
> **"적절한 파이프라인 최적화를 통해, 12B급 오픈소스 LLM(Gemma-3-12B)이 상용 API(GPT-4o-mini)보다 우수한 성능을 달성할 수 있다."**

### 청중이 기억해야 할 3가지
1. **Hybrid RRF 검색**이 한국어 행정문서에서 최적 (VectorDB 대비 +54% F1)
2. 높은 초기 Recall 환경에서 **리랭커는 비용 대비 효과 없음**
3. **On-premise 12B LLM이 상용 API 능가 가능** (SemScore +7.7%)

---

## 2. Motivation (연구 동기)

### 2.1 문제 상황 (Problem Statement)

#### LLM의 본질적 한계
| 한계 | 설명 | 공공행정 영향 |
|------|------|---------------|
| **환각(Hallucination)** | 사실과 다른 정보를 그럴듯하게 생성 | 잘못된 민원 응대, 법적 책임 |
| **지식 기준일 제약** | 학습 시점 이후 정보 반영 불가 | 최신 정책/법령 정보 부재 |
| **도메인 지식 부족** | 행정 전문용어, 법령 이해 부족 | 부정확한 전문 답변 |
| **출처 불명확** | 답변 근거 확인 불가 | 신뢰성/책임성 문제 |

#### 공공기관의 특수한 제약
```
┌─────────────────────────────────────────────────────────────┐
│                    공공기관 제약 조건                        │
├─────────────────────────────────────────────────────────────┤
│ 1. 데이터 주권 (Data Sovereignty)                            │
│    - 민원 데이터, 행정 정보의 외부 전송 불가                  │
│    - 클라우드 API 사용 제한                                  │
│                                                             │
│ 2. 보안 요구사항                                             │
│    - 폐쇄망 환경 운용 필요                                   │
│    - 외부 서비스 의존성 최소화                               │
│                                                             │
│ 3. 비용 효율성                                               │
│    - API 토큰 비용 부담                                      │
│    - 지속가능한 운용 모델 필요                               │
└─────────────────────────────────────────────────────────────┘
```

#### 연구 공백 (Research Gap)
- 한국어 공공 도메인 RAG 파이프라인 **체계적 비교 연구 부재**
- 기존 연구는 법률/의학 등 **특정 도메인 편중**
- **On-premise 오픈소스 LLM**과 상용 API의 정량적 비교 부족

### 2.2 해결 방안 (Proposed Solution)

**RAG (Retrieval-Augmented Generation)**
- 외부 지식원에서 관련 정보를 검색 → LLM에 컨텍스트로 제공 → 답변 생성
- 환각 감소, 최신 정보 반영, 출처 명시 가능

```
Query → [검색(Retrieval)] → [재랭킹(Reranking)] → [생성(Generation)] → Answer
              ↓                    ↓                      ↓
         외부 지식원            순위 조정               LLM 생성
```

---

## 3. Research Questions & Hypotheses

### 3.1 Main Research Question

> **Main RQ**: 한국어 공공 도메인에서, 파이프라인 최적화를 통해 On-premise 오픈소스 LLM이 상용 API 대비 **통계적으로 동등한 성능**을 달성할 수 있는가?

### 3.2 세부 연구 질문 (Sub-RQs)

| RQ | 질문 | 평가 대상 |
|----|------|-----------|
| **RQ1** | 한국어 공공 도메인에서 BM25, VectorDB, Hybrid 검색의 성능 차이는? | 검색 방식 |
| **RQ2** | 한국어 특화 리랭커(KoReranker)가 범용 리랭커 대비 유의미한 향상을 제공하는가? | 리랭커 |
| **RQ3** | On-premise 오픈소스 LLM이 GPT-4o-mini 대비 어느 수준의 성능을 달성하는가? | LLM |

### 3.3 연구 가설 (Hypotheses)

| 가설 | 내용 | 검증 지표 |
|------|------|-----------|
| **H1** | Hybrid RRF가 단일 검색(BM25/VectorDB) 대비 높은 Retrieval F1을 달성한다 | F1, Recall |
| **H2** | 한국어 특화 리랭커(KoReranker)가 범용 리랭커 대비 유의미한 성능 향상을 제공한다 | F1, Recall |
| **H3** | 최적 RAG 파이프라인이 적용된 12B-32B급 오픈소스 LLM이 GPT-4o-mini와 통계적으로 동등한 성능을 달성한다 | SemScore, METEOR |

---

## 4. 연구 기여 (Contributions)

### 4.1 학술적 기여

| # | 기여 | 설명 |
|---|------|------|
| **C1** | 한국어 공공도메인 RAG 벤치마크 최초 구축 | 180 QA × 50,000 corpus (정답:Distractor = 1:244) |
| **C2** | On-premise RAG 최적 파이프라인 도출 | AutoRAG 기반 Greedy 탐색 (95.6% 비용 절감) |
| **C3** | 오픈소스 LLM 경쟁력 정량적 실증 | Gemma-3-12B > GPT-4o-mini (SemScore +7.7%) |
| **C4** | 공공기관 도입 청사진 제공 | RTX 3090Ti 24GB 환경에서 구현 가능 |

### 4.2 실무적 시사점

1. **데이터 주권 유지하면서 상용 API 수준 성능 달성 가능**
2. **리랭커 없이도 높은 성능** → 초기 구축 비용 절감
3. **24GB VRAM GPU로 12B 모델 운용 가능** → 공공기관 접근 가능한 하드웨어

---

## 5. 슬라이드 목차 구성

### 5.1 전체 흐름: Motivation → RQ/H → Method → Results → Contribution

```
┌──────────────────────────────────────────────────────────────┐
│                     발표 흐름 (Flow)                         │
├──────────────────────────────────────────────────────────────┤
│                                                              │
│  [1. WHY]        [2. WHAT]       [3. HOW]      [4. SO WHAT]  │
│  Motivation  →   RQ & H     →   Method    →   Results &     │
│                                               Contribution   │
│                                                              │
│  "왜 이 연구?"   "무엇을 검증?"  "어떻게?"    "무엇을 발견?"  │
│                                                              │
└──────────────────────────────────────────────────────────────┘
```

### 5.2 슬라이드 목차 (22장 + 부록)

#### Part 1: Introduction (5장)
| # | 제목 | 내용 | 시간 |
|---|------|------|------|
| 1 | 표지 | 논문 제목, 저자, 소속, 날짜 | - |
| 2 | 목차 | 발표 구조 개요 | 30초 |
| 3 | **Motivation: 문제 상황** | LLM 한계, 공공기관 제약 | 1분 |
| 4 | **Motivation: 연구 공백** | 선행연구 한계, 본 연구 필요성 | 1분 |
| 5 | **RQ & Hypotheses** | Main RQ, Sub-RQs, H1/H2/H3 | 1분 |

#### Part 2: Research Method (5장)
| # | 제목 | 내용 | 시간 |
|---|------|------|------|
| 6 | RAG 개념 | RAG 파이프라인 구조, 장점 | 1분 |
| 7 | 데이터셋 구축 | AI Hub 데이터, 180 QA, 50K corpus | 1분 |
| 8 | 질문 유형 분류 | 7가지 유형 (Simple~Multi-hop 5) | 30초 |
| 9 | 파이프라인 구성 | AutoRAG, Greedy Modular, 탐색 공간 | 1분 |
| 10 | 실험 환경 | 하드웨어, 소프트웨어, 평가 지표 | 30초 |

#### Part 3: Results & Hypothesis Verification (8장)
| # | 제목 | 내용 | 시간 |
|---|------|------|------|
| 11 | **H1 결과: 검색 성능** | 그래프 + Hybrid RRF 우수성 | 1분 |
| 12 | **H1 검증** | 통계 검정 결과, 부분 채택 | 1분 |
| 13 | **H2 결과: 리랭커** | 그래프 + 6개 리랭커 동일 성능 | 1분 |
| 14 | **H2 검증** | 기각 이유, 해석 | 30초 |
| 15 | **H3 결과: LLM 비교** | 그래프 + Gemma-3-12B 최고 성능 | 1분 |
| 16 | **H3 검증** | 통계 검정, 부분 채택 | 1분 |
| 17 | Ablation Study | No-RAG → Naive → Optimized | 1분 |
| 18 | 핵심 모델 비교 | GPT-4o-mini vs Gemma-3-12B 심층 비교 | 1분 |

#### Part 4: Conclusion (4장)
| # | 제목 | 내용 | 시간 |
|---|------|------|------|
| 19 | 가설 검증 요약 | H1/H2/H3 결과 한눈에 | 30초 |
| 20 | 최적 파이프라인 | 도출된 최적 구성, Naive 대비 향상 | 1분 |
| 21 | **Contributions** | C1~C4 학술적/실무적 기여 | 1분 |
| 22 | 한계 및 향후 연구 | 데이터 규모, 리랭커, 확장 방향 | 1분 |
| 23 | 감사 & Q&A | 마무리 | - |

**총 예상 시간: 약 18-20분**

---

## 6. 부록 슬라이드 (Appendix)

### 6.1 부록 A: 상세 실험 결과

#### A-1. 검색 방식별 상세 결과
| 검색 방식 | Precision | Recall | F1 | MRR |
|-----------|-----------|--------|-----|-----|
| BM25 | 0.284 | 0.794 | 0.363 | 0.528 |
| VectorDB | 0.185 | 0.633 | 0.246 | 0.374 |
| **Hybrid RRF** | **0.287** | **0.850** | **0.379** | **0.560** |

#### A-2. LLM별 전체 지표 결과
| LLM | SemScore | BERTScore | ROUGE-L | METEOR |
|-----|----------|-----------|---------|--------|
| **Gemma-3-12B** | **0.532** | **0.727** | **0.341** | **0.235** |
| EXAONE-32B (Q4) | 0.504 | 0.691 | 0.317 | 0.208 |
| GPT-4o-mini | 0.494 | 0.697 | 0.338 | 0.206 |
| A.X-4.0-Light-7B | 0.482 | 0.647 | 0.271 | 0.176 |
| HyperCLOVAX-SEED-1.5B | 0.458 | 0.606 | 0.227 | 0.149 |
| GPT-OSS-20B | 0.380 | 0.613 | 0.244 | 0.153 |

#### A-3. 질문 유형별 성능
| 질문 유형 | Recall | SemScore |
|-----------|--------|----------|
| Simple Factoid | 94.4% | 0.547 |
| Constraint | 91.7% | 0.539 |
| Reasoning | 88.9% | 0.521 |
| Multi-doc 1-hop | 83.3% | 0.512 |
| Multi-hop 2-hop | 80.6% | 0.498 |
| Multi-hop 3-hop | 72.2% | 0.473 |
| Multi-hop 5-hop | 55.6% | 0.421 |

### 6.2 부록 B: 통계 검정 상세

#### B-1. H1 검증 통계
| 비교 | t-statistic | p-value | Cohen's d | 95% CI |
|------|-------------|---------|-----------|--------|
| Hybrid vs VectorDB | 8.57 | <0.0001 | 0.639 (medium) | [0.098, 0.167] |
| Hybrid vs BM25 | 1.68 | 0.095 | 0.072 (negligible) | [-0.003, 0.035] |

#### B-2. H3 검증 통계 (Bonferroni 보정 α'=0.01)
| 비교 (vs GPT-4o-mini) | p-value | Cohen's d | 판정 |
|----------------------|---------|-----------|------|
| Gemma-3-12B | 0.0007 | 0.169 | 유의미하게 우수 |
| EXAONE-32B | 0.344 | 0.045 | 동등 |
| A.X-4.0-Light | 0.189 | 0.063 | 동등 |
| HyperCLOVAX | 0.021 | -0.158 | 유의미하지 않음 |
| GPT-OSS-20B | <0.0001 | -0.582 | 유의미하게 열등 |

### 6.3 부록 C: 실험 환경 상세

```
하드웨어:
- GPU: NVIDIA RTX 3090 Ti (24GB VRAM)
- CPU: Intel Core i5-11400
- RAM: 32GB DDR4
- Storage: NVMe SSD 2TB

소프트웨어:
- OS: Ubuntu 24.04 LTS
- Python: 3.11
- CUDA: 12.4
- AutoRAG: v0.3.21
- Ollama: v0.11.7

재현성 조건:
- Random Seed: 42
- LLM Temperature: 0
- Embedding: OpenAI text-embedding-3-small
```

---

## 7. 예상 질문 및 답변 (Q&A 대비)

### 7.1 연구 방법론 관련

#### Q1: 180개 QA가 너무 적은 것 아닌가?
> **A**: 본 연구는 탐색적 연구(Pilot Study)로서 방법론적 타당성을 검증하는 것이 목표입니다. 비교 대상 연구들(KBL benchmark 1,000 QA, SDS VDR 600 QA)과 유사한 규모이며, 6개 토픽 × 30개 질문으로 균형 있는 분포를 확보했습니다. 통계 검정 결과 주요 결론(H1, H3)에서 유의미한 결과를 얻었으며, 향후 연구에서 500-1,000개로 확장할 계획입니다.

#### Q2: Greedy Modular 방식의 한계는?
> **A**: Greedy 방식은 전역 최적해를 보장하지 못하고, 구성요소 간 상호작용 효과를 포착하지 못합니다. 이를 완화하기 위해:
> 1. Ablation Study로 각 구성요소 기여도 검증
> 2. 단일 메트릭이 아닌 평균 순위로 선택
> 3. 결론에서 한계를 명확히 기술
>
> 전체 432개 조합 탐색(Full Factorial)은 비용 문제로 수행하지 못했으며, 향후 연구 과제입니다.

#### Q3: 왜 SemScore를 주요 지표로 사용했는가?
> **A**:
> 1. RAG의 목적은 "정확한 정보 전달"이지 "단어 일치"가 아님
> 2. 한국어는 어순 자유, 조사 변화가 많아 n-gram 매칭(METEOR/ROUGE)이 불리
> 3. 같은 의미를 다른 표현으로 답해도 SemScore는 높게 평가
> 4. 2024-2025 RAG 논문 동향: 의미 기반 평가(semantic similarity)가 주류

### 7.2 실험 결과 관련

#### Q4: 리랭커가 효과 없다는 것이 일반화 가능한가?
> **A**: **아닙니다.** 본 연구의 결과는 "높은 초기 Recall(85%) 환경에서 리랭커의 추가 효과가 제한적"임을 보여줍니다. 이는:
> - 50,000개 코퍼스에서 Hybrid RRF가 이미 우수한 검색 품질 달성
> - top-k=5에서 정답 문서가 대부분 포함
>
> 코퍼스가 수십만~수백만으로 확장되거나, 초기 검색 품질이 낮은 환경에서는 리랭커 효과가 달라질 수 있습니다. 향후 연구에서 검증 예정입니다.

#### Q5: Gemma-3-12B가 GPT-4o-mini보다 우수한 이유는?
> **A**: 명확한 인과관계를 규명하기는 어렵지만, 두 가지 가능성을 제시합니다:
> 1. **RAG 환경에서의 컨텍스트 활용 능력**: No-RAG에서는 GPT-4o-mini가 우수했으나, RAG 적용 후 Gemma가 역전 → Gemma가 검색된 컨텍스트를 더 효과적으로 활용
> 2. **한국어 처리 능력**: Gemma-3는 다국어 학습에서 한국어 비중이 높을 가능성
>
> 이는 추측이며, 향후 연구에서 프롬프트 변형, 컨텍스트 길이 변화 등으로 검증 필요합니다.

#### Q6: EXAONE-32B가 양자화(Q4)로 인해 성능이 저하된 것 아닌가?
> **A**: 가능성이 있습니다. 24GB VRAM 제약으로 4비트 양자화를 적용했으며, 이로 인한 성능 저하를 배제할 수 없습니다. 그러나:
> 1. EXAONE-32B (Q4)도 GPT-4o-mini와 통계적으로 동등한 성능 달성
> 2. 실제 공공기관 환경에서도 VRAM 제약이 존재하므로, 현실적인 조건에서의 평가로 의의 있음
>
> 향후 48GB 이상 VRAM 환경에서 FP16 추론 비교가 필요합니다.

### 7.3 적용 및 확장 관련

#### Q7: 실제 공공기관 적용 시 고려사항은?
> **A**:
> 1. **코퍼스 규모 확장**: 실제 환경은 수십만 문서 → 리랭커 효과 재검증 필요
> 2. **실시간 요구사항**: 응답 시간 요구에 따라 모델 선택 (Gemma-3-12B: 0.66초)
> 3. **보안 인증**: On-premise 환경 구축, 네트워크 격리
> 4. **지속적 업데이트**: 지식베이스 갱신 파이프라인 구축

#### Q8: GraphRAG와의 비교는?
> **A**: 본 연구는 벡터 기반 RAG에 초점을 두었으며, GraphRAG는 향후 연구 과제입니다.
> - Multi-hop 5-hop 유형에서 성능이 급격히 하락(Recall 55.6%)한 점은 현재 벡터 RAG의 한계
> - 지식그래프 기반 검색은 문서 간 관계를 명시적으로 모델링하여 복잡한 다단계 추론에 효과적일 수 있음
> - 한국어 공공 도메인에서 GraphRAG vs VectorRAG 비교가 의미 있는 연구 방향

### 7.4 선행연구 관련

#### Q9: 해외 연구와의 차별점은?
> **A**:
> 1. **도메인**: 영어 법률/의학 → 한국어 공공행정
> 2. **평가 범위**: 검색 또는 생성 단독 → 전체 파이프라인 통합 평가
> 3. **LLM**: GPT 중심 → On-premise 오픈소스 LLM 비교
> 4. **실용성**: 연구 목적 → 공공기관 도입 가능한 청사진 제공

#### Q10: 한국어 RAG 선행연구(정상무, 권혁규, 이채원)와의 차이는?
> **A**:
> | 연구 | 도메인 | 검색 | 리랭커 | LLM 비교 |
> |------|--------|------|--------|----------|
> | 정상무(2024) | 행정문서 요약 | - | - | 1종 |
> | 권혁규(2025) | 대학 규정 | BM25+Vector | - | 2종 |
> | 이채원(2025) | 한국어 일반 | Hybrid | - | 3종 |
> | **본 연구** | 공공행정 | **3종 비교** | **6종 비교** | **6종 비교** |
>
> 본 연구는 가장 체계적이고 포괄적인 파이프라인 비교를 수행했습니다.

---

## 8. 발표 시 강조 포인트

### 8.1 시각적 강조

1. **검색 결과 그래프**: Hybrid RRF의 압도적 우위 (색상 대비)
2. **LLM 비교 그래프**: Gemma-3-12B > GPT-4o-mini (승자 강조)
3. **Ablation Study**: No-RAG → RAG의 역전 현상 (화살표 애니메이션)

### 8.2 스토리텔링

```
[문제 제기] "공공기관이 LLM을 도입하려면 데이터 주권 문제가 있습니다"
     ↓
[질문 던지기] "과연 On-premise 오픈소스 LLM으로 상용 API 수준을 달성할 수 있을까요?"
     ↓
[실험 수행] "체계적인 파이프라인 비교 실험을 수행했습니다"
     ↓
[결과 발표] "놀랍게도 12B 오픈소스 모델이 상용 API를 능가했습니다"
     ↓
[시사점] "공공기관도 데이터 주권을 지키면서 고성능 RAG를 구축할 수 있습니다"
```

### 8.3 핵심 숫자 강조

| 항목 | 숫자 | 의미 |
|------|------|------|
| **+54.1%** | Hybrid vs VectorDB F1 향상 | 검색 방식 선택의 중요성 |
| **+7.7%** | Gemma vs GPT-4o-mini SemScore | 오픈소스 LLM의 경쟁력 |
| **+43.8%** | Gemma No-RAG → Optimized 향상 | RAG의 효과 |
| **95.6%** | Greedy 방식 비용 절감 | 효율적인 탐색 방법 |
| **0원** | API 비용 | On-premise의 경제적 이점 |

---

## 9. 발표 자료 체크리스트

### 9.1 필수 포함 그래프/테이블
- [ ] 검색 방식별 성능 비교 그래프 (fig4_1)
- [ ] 리랭커별 성능 비교 그래프 (fig4_9)
- [ ] LLM별 SemScore 비교 그래프 (fig4_2)
- [ ] GPT vs Gemma 다중 지표 비교 (fig4_8)
- [ ] Ablation Study 그래프 (fig4_4)
- [ ] 가설 검증 요약 테이블
- [ ] 최적 파이프라인 구성 테이블

### 9.2 발표 전 확인
- [ ] 모든 그래프 해상도 확인
- [ ] 통계 수치 정확성 검증
- [ ] 발표 시간 리허설 (18-20분 목표)
- [ ] Q&A 대비 부록 자료 준비

---

*Last Updated: 2025-12-08*
