import OpenAI from 'openai';

const DEFAULT_MODEL = 'gpt-4.1-mini';
const API_KEY_STORAGE_KEY = 'openai_api_key';

// localStorageì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
function getApiKey(): string {
  if (typeof window !== 'undefined') {
    return localStorage.getItem(API_KEY_STORAGE_KEY) || '';
  }
  return '';
}

let openaiClient: OpenAI | null = null;
let cachedApiKey: string = '';

function getClient(): OpenAI {
  const apiKey = getApiKey();
  if (!apiKey) {
    throw new Error('API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìƒë‹¨ ë©”ë‰´ì—ì„œ API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.');
  }
  // API í‚¤ê°€ ë³€ê²½ë˜ë©´ í´ë¼ì´ì–¸íŠ¸ ì¬ìƒì„±
  if (!openaiClient || cachedApiKey !== apiKey) {
    cachedApiKey = apiKey;
    openaiClient = new OpenAI({
      apiKey,
      dangerouslyAllowBrowser: true,
    });
  }
  return openaiClient;
}

export interface ModelInfo {
  id: string;
  name: string;
  description: string;
  inputTokenLimit?: number;
  outputTokenLimit?: number;
}

export function setApiKey(apiKey: string): void {
  if (typeof window !== 'undefined') {
    if (apiKey) {
      localStorage.setItem(API_KEY_STORAGE_KEY, apiKey);
    } else {
      localStorage.removeItem(API_KEY_STORAGE_KEY);
    }
    openaiClient = null;
    cachedApiKey = '';
  }
}

export function clearApiKey(): void {
  setApiKey('');
}

export function getSelectedModel(): string {
  return DEFAULT_MODEL;
}

export function setSelectedModel(_modelId: string): void {
  // ê³ ì • ëª¨ë¸ ì‚¬ìš©
}

export async function fetchAvailableModels(): Promise<ModelInfo[]> {
  return [{
    id: DEFAULT_MODEL,
    name: 'GPT-4.1 Mini',
    description: 'OpenAI GPT-4.1 Mini model',
  }];
}

export function getCachedModels(): ModelInfo[] {
  return [{
    id: DEFAULT_MODEL,
    name: 'GPT-4.1 Mini',
    description: 'OpenAI GPT-4.1 Mini model',
  }];
}

export interface Message {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface CompletionOptions {
  model?: string;
  temperature?: number;
  maxTokens?: number;
  onChunk?: (chunk: string) => void;
}

function isRateLimitError(error: unknown): boolean {
  if (error instanceof Error) {
    const message = error.message.toLowerCase();
    return (
      message.includes('rate limit') ||
      message.includes('rate_limit') ||
      message.includes('quota') ||
      message.includes('429') ||
      message.includes('too many requests')
    );
  }
  return false;
}

function getErrorMessage(error: unknown): string {
  if (isRateLimitError(error)) {
    return 'â³ API ìš”ì²­ í•œë„ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.\n\nì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.';
  }

  if (error instanceof Error) {
    if (error.message.includes('API key') || error.message.includes('api_key') || error.message.includes('401') || error.message.includes('Incorrect API key')) {
      return 'ğŸ”‘ API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\nì˜¬ë°”ë¥¸ OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.';
    }
    if (error.message.includes('network') || error.message.includes('fetch')) {
      return 'ğŸŒ ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.\n\nì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.';
    }
    return error.message;
  }

  return 'ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.';
}

export async function streamCompletion(
  messages: Message[],
  options: CompletionOptions = {}
): Promise<string> {
  const client = getClient();
  const { model = DEFAULT_MODEL, temperature = 0.7, maxTokens = 8192, onChunk } = options;

  try {
    const stream = await client.chat.completions.create({
      model,
      messages: messages.map(m => ({
        role: m.role,
        content: m.content,
      })),
      temperature,
      max_tokens: maxTokens,
      stream: true,
    });

    let fullContent = '';

    for await (const chunk of stream) {
      const text = chunk.choices[0]?.delta?.content || '';
      if (text) {
        fullContent += text;
        onChunk?.(text);
      }
    }

    return fullContent;
  } catch (error) {
    throw new Error(getErrorMessage(error));
  }
}

export async function getCompletion(
  messages: Message[],
  options: Omit<CompletionOptions, 'onChunk'> = {}
): Promise<string> {
  const client = getClient();
  const { model = DEFAULT_MODEL, temperature = 0.7, maxTokens = 8192 } = options;

  try {
    const response = await client.chat.completions.create({
      model,
      messages: messages.map(m => ({
        role: m.role,
        content: m.content,
      })),
      temperature,
      max_tokens: maxTokens,
    });

    return response.choices[0]?.message?.content || '';
  } catch (error) {
    throw new Error(getErrorMessage(error));
  }
}

export function isApiKeyConfigured(): boolean {
  return !!getApiKey();
}

export function getStoredApiKey(): string {
  return getApiKey();
}

export function getModelName(): string {
  return 'GPT-4.1 Mini';
}

export async function validateApiKey(apiKey: string): Promise<boolean> {
  if (!apiKey || !apiKey.startsWith('sk-')) {
    return false;
  }

  try {
    const testClient = new OpenAI({
      apiKey,
      dangerouslyAllowBrowser: true,
    });

    // ê°„ë‹¨í•œ API í˜¸ì¶œë¡œ ê²€ì¦
    await testClient.models.list();
    return true;
  } catch {
    return false;
  }
}
